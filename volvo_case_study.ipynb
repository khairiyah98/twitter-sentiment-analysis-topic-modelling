{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4215,
     "status": "ok",
     "timestamp": 1617109533647,
     "user": {
      "displayName": "Khai riyah",
      "photoUrl": "",
      "userId": "06191879402753524752"
     },
     "user_tz": -480
    },
    "id": "bNkXwR9r9BrN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import gensim\n",
    "import re\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('volvo_nlp.txt') as file:\n",
    "    lines = [line.rstrip() for line in file]\n",
    "\n",
    "#drop empty lines\n",
    "lines2 = [x for x in lines if x]\n",
    "\n",
    "lines2\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "lines3 = []\n",
    "for k in lines2:\n",
    "    str_output = re.sub('â€™', '\\'', k)\n",
    "    #print(str_output)\n",
    "    lines3.append(str_output)\n",
    "\n",
    "len(lines3)\n",
    "\n",
    "lines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df= pd.DataFrame()\n",
    "\n",
    "tweets_df['Text']=pd.Series(lines3)\n",
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install clean_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3553,
     "status": "ok",
     "timestamp": 1617109578571,
     "user": {
      "displayName": "Khai riyah",
      "photoUrl": "",
      "userId": "06191879402753524752"
     },
     "user_tz": -480
    },
    "id": "4ZsXequj9Bru",
    "outputId": "fa3044db-3e93-44d0-c433-37fc181438cf"
   },
   "outputs": [],
   "source": [
    "# Import customs module to tokenize and clean tweet dataset\n",
    "import sys\n",
    "sys.path.insert(0, 'topic_modelling/notebooks/')\n",
    "from clean_tokenizer import tokenize_tweets, clean_tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aulcpCktghGu"
   },
   "source": [
    "### Frequency of tweets daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPQ8cdXk9Br3"
   },
   "source": [
    "### Preprocessing: Dictionary and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 651
    },
    "executionInfo": {
     "elapsed": 5780,
     "status": "ok",
     "timestamp": 1617109592820,
     "user": {
      "displayName": "Khai riyah",
      "photoUrl": "",
      "userId": "06191879402753524752"
     },
     "user_tz": -480
    },
    "id": "vP_8mv5jWvZn",
    "outputId": "ea7c83f6-6157-4e96-be21-c6f55333e801"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "# Convert cleaned tweet into tokens list\n",
    "tweets_df['Text']=tweets_df['Text'].apply(str)\n",
    "tweets_df['Text'] = tweets_df['Text'].replace({'govt':'government', 'Govt':'Government'} )\n",
    "tweets_df['clean_text_new']=tweets_df['Text'].apply(clean_tweet, bigrams=True) \n",
    "#   Note: clean_tweet function will: \n",
    "# -\tremove retweet and @user information\n",
    "# -\tremove web links\n",
    "# -\tremove hashtags\n",
    "# -\tremove audio/video tags or labels\n",
    "# -\tlower case the tweet\n",
    "# -\tstrip punctuation\n",
    "# -\tremove double spacing\n",
    "# -\tremove numbers\n",
    "# -\tapply lemmatization and tokenization (within lemmatize function, remove stop words drops words with 3 or less characters\n",
    "# -\tform bigrams\n",
    "tweets_df.head()\n",
    "tweets_df['clean_tokens'] = tweets_df.clean_text_new.apply(lambda x: re.split('\\s', x))\n",
    "#remove additional custom stopwords\n",
    "# stop = []\n",
    "stop = [\"pm\",\"t.co\",\"http\",\"https\",\"amp\",\"t\",\"t.c\",\"c\",\"rt\", \"pl\", \"s\", \"p\", \"like\", \"im\",\"new\", \"day\", \"days\",\"year\", \"ur\", \"ve\", \"la\", \"ive\", \"cos\", \"guys\",\n",
    "        \"didnt\", \"time\", \"people\", \"dont\", \"today\", \"thing\", \"week\", \"months\", \"post\",\"yesterday\", \"man\", \"wont\", \"uk\",\n",
    "        \"st\", \"lets\", \"don\", \"feel\", \"gonna\",\"isnt\", \"pls\", \"share\", \"wait\", \"wanna\", \"na\", \"back\", \"means\",\n",
    "        \"lah\",\"due\", \"sa\", \"ingat\", \"just\", \"will\", \"can\", \"now\", \"get\", \"go\", \"us\",\n",
    "        \"can\", \"one\", \"even\", \"just\", \"ada\", \"ke\", \"got\", \"going\", \"last\", \"etc\", \"kaypo\", \"still\", \"say\", \"know\",\n",
    "        \"situation\", \"need\", \"want\", \"take\", \"come\", \"look\",\n",
    "        \"think\", 'actually', 'especially', 'later', 'guess', 'note', 'dear', 'road', 'start', 'stop', 'things', 'give',\n",
    "        'try', 'tell', 'shit', 'maybe', 'keep', 'right']\n",
    "tweets_df['clean_tokens_final']= tweets_df['clean_tokens'].apply(lambda x: [item for item in x if item not in stop])\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2006,
     "status": "ok",
     "timestamp": 1617109597198,
     "user": {
      "displayName": "Khai riyah",
      "photoUrl": "",
      "userId": "06191879402753524752"
     },
     "user_tz": -480
    },
    "id": "goIUzubt_Ics",
    "outputId": "08d0e3ba-a0d5-4398-b0d7-779e2cc3cad3"
   },
   "outputs": [],
   "source": [
    "all_words = [word for item in list(tweets_df['clean_tokens_final']) for word in item]\n",
    "#all_words\n",
    "\n",
    "#frequency distribution of all terms\n",
    "fdist = nltk.FreqDist(all_words)\n",
    "\n",
    "#number of unique terms\n",
    "len(fdist)\n",
    "#fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2142,
     "status": "ok",
     "timestamp": 1617109599811,
     "user": {
      "displayName": "Khai riyah",
      "photoUrl": "",
      "userId": "06191879402753524752"
     },
     "user_tz": -480
    },
    "id": "L0TPT5-s_vCh"
   },
   "outputs": [],
   "source": [
    "#create dataframe of terms with their respective frequency\n",
    "fdist = pd.DataFrame.from_dict(fdist, orient='index').reset_index()\n",
    "fdist.columns = ['word', 'freq']\n",
    "fdist = fdist.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2521,
     "status": "ok",
     "timestamp": 1617109602777,
     "user": {
      "displayName": "Khai riyah",
      "photoUrl": "",
      "userId": "06191879402753524752"
     },
     "user_tz": -480
    },
    "id": "CtXd9R7Y_5VI"
   },
   "outputs": [],
   "source": [
    "#discard words containing only 1 character\n",
    "fdist = fdist[fdist.apply(lambda r: len(r['word']) > 1, axis=1)]\n",
    "\n",
    "#discard numbers/digits\n",
    "fdist = fdist[fdist.apply(lambda r: r['word'].isdigit() == False, axis=1)]\n",
    "\n",
    "#select terms that appear at least 2 times\n",
    "fdist = fdist[(fdist.freq >= 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "executionInfo": {
     "elapsed": 1933,
     "status": "ok",
     "timestamp": 1617109602783,
     "user": {
      "displayName": "Khai riyah",
      "photoUrl": "",
      "userId": "06191879402753524752"
     },
     "user_tz": -480
    },
    "id": "c4w8dQg-_7M0",
    "outputId": "aadf19a8-10e7-429b-c611-4af18c625d77"
   },
   "outputs": [],
   "source": [
    "fdist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 903,
     "status": "ok",
     "timestamp": 1617109604251,
     "user": {
      "displayName": "Khai riyah",
      "photoUrl": "",
      "userId": "06191879402753524752"
     },
     "user_tz": -480
    },
    "id": "B5axjrV1e_WK",
    "outputId": "73777167-a81a-46f1-c53a-e712dba30777"
   },
   "outputs": [],
   "source": [
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1457,
     "status": "ok",
     "timestamp": 1617109605295,
     "user": {
      "displayName": "Khai riyah",
      "photoUrl": "",
      "userId": "06191879402753524752"
     },
     "user_tz": -480
    },
    "id": "q9ZoGe1YAzmK"
   },
   "outputs": [],
   "source": [
    "#dictionary to collect order of mapping\n",
    "dict_word = {}\n",
    "for i in range(len(fdist)):\n",
    "    temp = fdist.iloc[i]\n",
    "    word = temp['word']\n",
    "    dict_word[word] = word\n",
    "\n",
    "\n",
    "#mapping\n",
    "tweets_df['cleaned'] = tweets_df.apply(lambda row: [dict_word[x] for x in row.clean_tokens_final if x in dict_word.keys() ], axis=1)\n",
    "\n",
    "#removing stopwords\n",
    "tweets_df['cleaned'] = tweets_df.apply(lambda row: [x for x in row.cleaned if x not in stop], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0U7ZjtIrfKV"
   },
   "source": [
    "# Hashtag analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 739,
     "status": "ok",
     "timestamp": 1617109625507,
     "user": {
      "displayName": "Khai riyah",
      "photoUrl": "",
      "userId": "06191879402753524752"
     },
     "user_tz": -480
    },
    "id": "zkb8mTd-sMph"
   },
   "outputs": [],
   "source": [
    "def get_hashtags(text):\n",
    "    list_of_hashtags = []\n",
    "    temp = text.split()\n",
    "    for word in temp:\n",
    "        if word[0] == '#':\n",
    "            list_of_hashtags.append(word)\n",
    "    return list_of_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1617109627070,
     "user": {
      "displayName": "Khai riyah",
      "photoUrl": "",
      "userId": "06191879402753524752"
     },
     "user_tz": -480
    },
    "id": "2HcgH51zsOAW"
   },
   "outputs": [],
   "source": [
    "texts = list(tweets_df['Text'])\n",
    "tweets_df['hashtags'] = [get_hashtags(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "executionInfo": {
     "elapsed": 1204,
     "status": "ok",
     "timestamp": 1617109629055,
     "user": {
      "displayName": "Khai riyah",
      "photoUrl": "",
      "userId": "06191879402753524752"
     },
     "user_tz": -480
    },
    "id": "vpT18Jq4reSb",
    "outputId": "84d6e1c9-7ac3-4586-cdcc-2729359faabc"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Feb\n",
    "all_hashtags = list(tweets_df['hashtags'])\n",
    "all_hashtags = Counter([item for sublist in all_hashtags for item in sublist])\n",
    "all_hashtags = pd.DataFrame.from_dict(all_hashtags, orient='index').reset_index()\n",
    "all_hashtags.columns = ['hashtag', 'num']\n",
    "all_hashtags['sum'] = np.sum(all_hashtags['num'])\n",
    "all_hashtags['pct'] = 100 * all_hashtags['num']/all_hashtags['sum']\n",
    "all_hashtags = all_hashtags.sort_values('pct', ascending = False).head(10)\n",
    "all_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparation\n",
    "texts = list(tweets_df['cleaned'])\n",
    "\n",
    "id2word = gensim.corpora.Dictionary(texts)\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "#choose no. of topics based on coherence score\n",
    "num_topics = np.arange(5,30+1, 2)\n",
    "coherences = []\n",
    "models = []\n",
    "\n",
    "for num_topic in num_topics:\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=num_topic, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=6000,\n",
    "                                           passes=20,\n",
    "                                           alpha='auto',\n",
    "                                           eta='auto',\n",
    "                                           eval_every=None)\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    coherences.append(coherence_score)\n",
    "    models.append(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coherence scores across topic numbers\n",
    "sns.reset_orig()\n",
    "sns.set(font_scale = 2)\n",
    "plotData = pd.DataFrame({'Number of topics':num_topics,\n",
    "                         'Coherence score':coherences})\n",
    "f,ax = plt.subplots(figsize=(16,10))\n",
    "#sns.set_style(\"darkgrid\")\n",
    "#sns.set(font_scale = 0.1)\n",
    "sns.pointplot(x='Number of topics', y= 'Coherence score',data=plotData)\n",
    "#plt.axhline(y=coherences[7], color='red', linestyle='--' )\n",
    "#plt.axvline(x=6, color='red', linestyle='--' )\n",
    "plt.title('Topic Coherence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module from gsdmm repository\n",
    "import sys\n",
    "sys.path.insert(0, 'topic_modelling/gsdmm/')\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "docs = tweets_df['cleaned'].tolist()\n",
    "vocab = set(x for doc in docs for x in doc)\n",
    "n_terms = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1000)\n",
    "mgp = MovieGroupProcess(K=21, alpha=0.1, beta=0.1, n_iters=40)\n",
    "y = mgp.fit(docs, n_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GSDMM model as pickle file\n",
    "\n",
    "import pickle\n",
    "\n",
    "filehandler = open(\"submit_model7.pkl\",\"wb\")\n",
    "pickle.dump(mgp,filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GSDMM model from saved pickle file\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('submit_model7.pkl', 'rb') as f:\n",
    " mgp = pickle.load(f)\n",
    " f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try saved model\n",
    "import operator\n",
    "\n",
    "for i in range(21):\n",
    "    print('Cluster ' + str(i))\n",
    "    print(sorted(mgp.cluster_word_distribution[i].items(),key = operator.itemgetter(1),reverse = True)[:10])\n",
    "    print('*' * 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(mgp.cluster_doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for i in range(len(docs)):\n",
    "    topics.append(mgp.choose_best_label(docs[i])[0])\n",
    "    \n",
    "tweets_df['sttm_topic_from_zero'] = topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift index of topics to +1 such that it starts from 0 instead of 1\n",
    "tweets_df['sttm_topic'] = tweets_df['sttm_topic_from_zero'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sttm_dist = tweets_df.groupby('sttm_topic').count().reset_index()\n",
    "sttm_dist['sum'] = np.sum(sttm_dist.index)\n",
    "sttm_dist['pct'] = 100 * sttm_dist.index / sttm_dist['sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(30, 15), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.rcParams.update({'font.size': 25})\n",
    "\n",
    "plt.bar(sttm_dist.sttm_topic, sttm_dist.pct, align='center', alpha=0.5, color='#66023C');\n",
    "plt.xlabel('Topic');\n",
    "plt.xticks(np.arange(np.min(sttm_dist.sttm_topic), np.max(sttm_dist.sttm_topic)+1, 1));\n",
    "plt.ylabel('Percentage of Tweets');\n",
    "plt.title('Distribution of Topics in the Tweets (GSDMM)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sttm_dist.sort_values(by=['pct'], ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sttm_dist['pct'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = tweets_df[(tweets_df['sttm_topic'] ==20)]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[Chapter 6.1.4] Exploratory data analysis.ipynb",
   "provenance": [
    {
     "file_id": "1xyAUpIcNDGWhO3rNhy31IDGgaJXUSSdj",
     "timestamp": 1617027944673
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
